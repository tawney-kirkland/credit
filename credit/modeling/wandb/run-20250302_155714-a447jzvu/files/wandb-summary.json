{"_step":4,"AUC":0.8269188663809302,"classification_report":"              precision    recall  f1-score   support\n\n         0.0       0.97      0.81      0.88     23336\n         1.0       0.20      0.69      0.32      1664\n\n    accuracy                           0.80     25000\n   macro avg       0.59      0.75      0.60     25000\nweighted avg       0.92      0.80      0.85     25000\n","confusion_matrix":{"_latest_artifact_path":"wandb-client-artifact://es9q64wqo277qoaqg0259bitl7445obe6rld1xwx9oq3vkpuf3os44501toohwho74ijrxnlsr1q00j8mgbrm9r0gnwdo73u94k3us9ljogzr55ikweiaxr6o2p8m7wx:latest/confusion_matrix.table.json","path":"media/table/confusion_matrix_4_ace728b041547147ee85.table.json","ncols":3,"nrows":4,"_type":"table-file","sha256":"ace728b041547147ee85a397e871ac49b5b823217966040ac5a1a62ad3bc5bf9","size":127,"artifact_path":"wandb-client-artifact://su8f8ultfn44mgtimdkkt4db9z6vqxyowf1swld0btq2lie7tf9aqw3n0ua9kwiay1gl87gpjvbgtfduzs5v12fh0yroh97lrnkgctcw0sz6q7vn6xtnjzishzm2jvtj/confusion_matrix.table.json"},"cv_accuracy":0.8022600000000001,"_wandb":{"runtime":28},"_timestamp":1.740949062316326e+09,"_runtime":28.144407}